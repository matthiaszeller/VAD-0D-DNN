{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipypublish import nb_setup\n",
    "# https://stackoverflow.com/a/39566040/11552622\n",
    "rcparams = {\n",
    "    'axes.titlesize':13,\n",
    "    'axes.labelsize':9,\n",
    "    'xtick.labelsize':8,\n",
    "    'ytick.labelsize':8\n",
    "}\n",
    "plt = nb_setup.setup_matplotlib(rcparams=rcparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Aim\n",
    "\n",
    "Implement framework to extract data generated by the script `script_stats_dnn.py` and concatenate it. This aims to process data from a single dataset. Further (but easy) manipulation are required to merge everything together (e.g. `pandas.join`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick implementation overview\n",
    "\n",
    "The script is runned from a folder containing `X.mat` and `Y.mat` files. It creates a new directory for each tested DNN configuration. For instance, the file hierarchy of the root folder once script is finished: \n",
    "\n",
    "```\n",
    ".\n",
    "├── 1_hlayers_16_neurons_40_aks\n",
    "├── 1_hlayers_32_neurons_40_aks\n",
    "├── 1_hlayers_64_neurons_40_aks\n",
    "├── 1_hlayers_8_neurons_40_aks\n",
    "├── 2_hlayers_16_neurons_40_aks\n",
    "├── 2_hlayers_32_neurons_40_aks\n",
    "├── 2_hlayers_64_neurons_40_aks\n",
    "├── 2_hlayers_8_neurons_40_aks\n",
    "├── 4_hlayers_16_neurons_40_aks\n",
    "├── 4_hlayers_32_neurons_40_aks\n",
    "├── 4_hlayers_64_neurons_40_aks\n",
    "├── 4_hlayers_8_neurons_40_aks\n",
    "├── 6_hlayers_16_neurons_40_aks\n",
    "├── 6_hlayers_32_neurons_40_aks\n",
    "├── 6_hlayers_64_neurons_40_aks\n",
    "├── 6_hlayers_8_neurons_40_aks\n",
    "├── X.mat\n",
    "└── Y.mat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each folder has the structure:\n",
    "\n",
    "```\n",
    "1_hlayers_16_neurons_40_aks/\n",
    "├── coefmaxs.npy\n",
    "├── coefmins.npy\n",
    "├── DNN_0D_Model.h5\n",
    "├── DNN_Performance.eps\n",
    "├── history.bin\n",
    "├── log.txt\n",
    "├── Losses.eps\n",
    "├── parammaxs.npy\n",
    "├── parammins.npy\n",
    "├── Ytestpred.txt\n",
    "└── Ytest.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in the files:\n",
    "\n",
    "* `history.bin`: a pickled dictionnary containing the keras model's history (losses)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/maousi/Data/tmp/dnn_stats/4000RPM_Pulse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare code for script \n",
    "\n",
    "## Aim\n",
    "\n",
    "Implement and test some routines that will be subsequently used in a script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walking around file hierarchy\n",
    "\n",
    "### Aim\n",
    "\n",
    "Given root folder, obtain the list of all files to load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(rootdir, fileformat):\n",
    "    \"\"\"\n",
    "    :param fileformat: example: `.txt`\n",
    "    \"\"\"\n",
    "    ls = []\n",
    "    for dirname, subdirlist, filelist in os.walk(rootdir):\n",
    "        for file in filelist:\n",
    "            if file.endswith(fileformat):\n",
    "                ls.append(os.path.join(dirname, file))\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list(path, '.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = get_list(path, '.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, extract, create DataFrame\n",
    "\n",
    "### Aim\n",
    "\n",
    "Given the list of history files, load them, extract the losses, generate DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine: parse dnn config from file path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config(filepath):\n",
    "    parent_dir = os.path.basename(os.path.dirname(filepath))\n",
    "    ls = parent_dir.split('_')\n",
    "    res = {\n",
    "        'hlayers': 0,\n",
    "        'neurons': 2,\n",
    "        'aks': 4\n",
    "    }\n",
    "    return {key: int(ls[index]) for key, index in res.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_config(ls[0]), parse_config(ls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine : Extract losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ls[0], 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['loss'])\n",
    "plt.plot(data['val_loss'])\n",
    "data['loss'][-1], data['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_losses(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # We extract the last item of every list\n",
    "    return {key: lst[-1] for key, lst in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_losses(ls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stats_data(rootdir):\n",
    "    files = get_list(rootdir, 'history.bin')\n",
    "    data = []\n",
    "    for f in files:\n",
    "        info = parse_config(f)\n",
    "        info.update(load_losses(f))\n",
    "        data.append(info)\n",
    "    return pd.DataFrame(data).sort_values(['hlayers', 'neurons', 'aks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_stats_data(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['val_loss']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot - Investigate loss <-> architecture\n",
    "\n",
    "### Aim \n",
    "\n",
    "Visualize losses in function of number of layers, number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dnn_stats(df):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    \n",
    "    ls_hlayers = df['hlayers'].value_counts('hlayers').index.values\n",
    "    for hlayers in ls_hlayers:\n",
    "        sub = df[df.hlayers == hlayers]\n",
    "        ax[0].plot(sub.neurons, sub.val_loss, '-o', label=str(hlayers))\n",
    "        ax[1].plot(sub.neurons, sub.val_mae, '-o')\n",
    "    \n",
    "    ax[0].legend(title='Nb layers')\n",
    "    ax[0].set_xlabel('Number of neurons')\n",
    "    ax[0].set_ylabel('Validation loss')\n",
    "    ax[1].set_xlabel('Number of neurons')\n",
    "    ax[1].set_ylabel('Mean absolute error')\n",
    "    plt.subplots_adjust(wspace=.4)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dnn_stats(df)\n",
    "plt.savefig('figs/dnn_architecture_stats.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dnn_architecture_analysis(basefolder, subfolders, suffixes):\n",
    "    for folder, suffix_name in zip(subfolders, suffixes):\n",
    "        folderpath = os.path.join(basefolder, folder)\n",
    "        print(folder)\n",
    "        \n",
    "        df = process_stats_data(folderpath)\n",
    "        plot_dnn_stats(df)\n",
    "        plt.savefig(f'figs/dnn_architecture_stats_{suffix_name}.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_dnn_architecture_analysis('/media/maousi/Data/tmp/dnn_stats', \n",
    "                                 ['4000RPM_Pulse', '5000RPM_Pulse', '6000RPM_Pulse'],\n",
    "                                 ['4000RPM_AP', '5000RPM_AP', '6000RPM_AP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = path+'/2_hlayers_32_neurons_40_aks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate and analyze results\n",
    "\n",
    "## Aim\n",
    "\n",
    "Combines `results.csv` files of all datasets and plot.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Run the script `script_stats_dnn.py --analyze` in each folder that is dedicated to a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/maousi/Data/tmp/dnn_stats/'\n",
    "!ls $path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine in a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = {e : os.path.join(path, e) for e in os.listdir(path)}\n",
    "folders = filter(lambda e: os.path.isdir(e[1]), folders.items())\n",
    "folders = dict(folders)\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys = RPM, value = results file path\n",
    "files = {int(name.split('RPM')[0]) : os.path.join(path, 'results.csv')\n",
    "         for name, path in folders.items()}\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for rpm, file in files.items():\n",
    "    tmp = pd.read_csv(file)\n",
    "    tmp['RPM'] = rpm\n",
    "    df.append(tmp)\n",
    "df = pd.concat(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.neurons.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset1(df):\n",
    "    # Hue = neurons\n",
    "    # x = number of coeffs\n",
    "    # y = loss\n",
    "    # separate plots = hlayers\n",
    "    \n",
    "    hue_var, plots = 'neurons', 'hlayers'\n",
    "    hue_values = df[hue_var].unique()\n",
    "    n_hue = len(hue_values)\n",
    "    hlayers_values = df.hlayers.unique()\n",
    "    n_plots = len(hlayers_values)\n",
    "    ncol = 2\n",
    "    x, y = 'aks', 'val loss'\n",
    "    colors = sns.color_palette()[:n_hue]\n",
    "    \n",
    "    df = df.rename({'val_loss' : 'val loss'}, axis=1).sort_values(x)\n",
    "    \n",
    "    fig, ax = plt.subplots(int(n_plots/ncol), ncol, sharex=True, sharey=True)\n",
    "    \n",
    "    for a, hlayers in zip(ax.ravel(), hlayers_values):\n",
    "        for i, hueval in enumerate(hue_values):\n",
    "            sub = df[df[hue_var] == hueval]\n",
    "            a.plot(sub[x], sub[y], color=colors[i])\n",
    "            a.scatter(sub[x], sub[y], s=12)\n",
    "    print(sub)\n",
    "    # x axis\n",
    "    for a in ax[-1]: a.set_xlabel(x)\n",
    "    #for a in ax[:-1].ravel(): a.set_xticks([])\n",
    "    # y axis\n",
    "    for a in ax[:, 0]: a.set_ylabel(y)\n",
    "    #for a in ax[:, 1:].ravel(): a.set_yticks([])\n",
    "    \n",
    "    plt.subplots_adjust(wspace=.07, hspace=.1)\n",
    "\n",
    "plot_dataset1(df[df.RPM == 4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dnn_stats(df):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    \n",
    "    ls_hlayers = df['hlayers'].unique()\n",
    "    colors = sns.color_palette()\n",
    "    for i, hlayers in enumerate(ls_hlayers):\n",
    "        sub = df[df.hlayers == hlayers]\n",
    "        ax[0].plot(sub.neurons, sub.val_loss, '-o', label=str(hlayers), color=colors[i])\n",
    "        #ax[0].plot(sub.neurons, sub.loss, '--o', color=colors[i])\n",
    "        ax[1].plot(sub.neurons, sub.val_loss - sub.loss, '-o', color=colors[i])\n",
    "    \n",
    "    ax[0].legend(title='Nb layers')\n",
    "    ax[0].set_xlabel('Number of neurons')\n",
    "    ax[0].set_ylabel('Validation loss')\n",
    "    ax[1].set_xlabel('Number of neurons')\n",
    "    ax[1].set_ylabel('Validation loss - training loss')\n",
    "    plt.subplots_adjust(wspace=.4)\n",
    "    plt.tight_layout()\n",
    "    ax[0].grid(ls='--'); ax[1].grid(ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dnn_stats(df[np.logical_and(df.RPM == 4000, df.aks == 40)])\n",
    "plt.suptitle('RPM = 4000', size=14)\n",
    "plot_dnn_stats(df[np.logical_and(df.RPM == 5000, df.aks == 40)])\n",
    "plt.suptitle('RPM = 5000', size=14)\n",
    "plot_dnn_stats(df[np.logical_and(df.RPM == 6000, df.aks == 40)])\n",
    "plt.suptitle('RPM = 6000', size=14)\n",
    "plt.savefig('figs/test.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average over 3 datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = df.groupby(['hlayers', 'neurons', 'aks']).mean().drop('RPM', axis=1)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = avg.reset_index().sort_values(['hlayers', 'neurons'])\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dnn_stats(avg[avg.aks == 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg[avg.aks == 40].sort_values('val_mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg[avg.aks == 40].sort_values('val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgtmp = avg[avg.aks == 40].sort_values('val_loss')\n",
    "print(avgtmp.to_latex(index=False, escape=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot effect of number of coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config(filepath):\n",
    "    parent_dir = os.path.basename(os.path.dirname(filepath))\n",
    "    ls = parent_dir.split('_')\n",
    "    \n",
    "    # values are indexes of element in `ls`\n",
    "    res = {\n",
    "        'hlayers': 0,\n",
    "        'neurons': 2,\n",
    "        'aks': 4\n",
    "    }\n",
    "    # Fill in with corresponding values\n",
    "    res = {key: int(ls[index]) for key, index in res.items()}\n",
    "    \n",
    "    # Get RPM\n",
    "    parent_parent_dir = os.path.dirname(os.path.dirname(filepath))\n",
    "    parent_parent_dir = os.path.basename(parent_parent_dir)\n",
    "    rpm = int(parent_parent_dir.split('RPM')[0])\n",
    "    res['RPM'] = rpm\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_list(path, 'history.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_config(files[0]), files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_config(files[-1]), files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# File structure: dictionnary\n",
    "# key = (RPM, layers, neurons, aks)\n",
    "# value = dictionnary {'loss': [...], 'val_loss': [...], ...}\n",
    "data = {}\n",
    "\n",
    "for f in files:\n",
    "    conf = parse_config(f)\n",
    "    key = (conf['RPM'], conf['hlayers'], conf['neurons'], conf['aks'])\n",
    "    \n",
    "    # Open the 'history' binary file containing losses and mae\n",
    "    with open(f, 'rb') as file:\n",
    "        data[key] = pickle.load(file)\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n",
    "\n",
    "Idea: plot losses and validation losses of all three data sets for a given DNN architecture and input size. Compute and display mean loss, validation loss, mae and validation mae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistics(data, configs, rpms, display_values, fun_descr):\n",
    "    nrow = len(configs)\n",
    "    ncol = len(rpms)\n",
    "    fig, axgrid = plt.subplots(nrow, ncol, figsize=(2.8*ncol + .5, 1.75*nrow + .5),\n",
    "                              sharey='row', sharex='col')\n",
    "    \n",
    "    for i, (axes, config) in enumerate(zip(axgrid, configs)):\n",
    "        # Plot data\n",
    "        last_val = {val: [] for val in display_values}\n",
    "        for rpm, a in zip(rpms, axes):\n",
    "            key = (rpm, ) + config\n",
    "            a.plot(data[key]['loss'], label='loss', lw=.5)\n",
    "            a.plot(data[key]['val_loss'], label='val loss', lw=.5)\n",
    "            # Extract last values\n",
    "            for val in display_values:\n",
    "                last_val[val].append(data[key][val][-1])\n",
    "            #last_val['loss'].append(data[key]['loss'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # X-Label management\n",
    "        # First row vs not\n",
    "        if i == 0:\n",
    "            axes[0].legend()\n",
    "            for rpm, a in zip(rpms, axes): a.set_title(f'{rpm} RPM')\n",
    "        # Last row vs not\n",
    "        #if i < nrow-1:\n",
    "        #    for a in axes: a.set_xticklabels([])\n",
    "        #else:\n",
    "        #    for a in axes: a.set_xlabel('Epochs')\n",
    "        \n",
    "        # Y-label management\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        #axes[-1].yaxis.set_label_position('right')\n",
    "        #axes[-1].yaxis.tick_right()\n",
    "        \n",
    "        #fig.align_ylabels(axes)\n",
    "        \n",
    "        # Text management - Compute mean values, display\n",
    "        text = '\\n'.join([\n",
    "            'mean ' + val.replace(\"_\", \" \") + ': ' + \\\n",
    "            '{:.3}'.format(np.mean(last_val[val]))\n",
    "            for val in display_values\n",
    "        ])\n",
    "        text = '\\\\textbf{'+fun_descr(config)+'}' + '\\n' + text\n",
    "        axes[0].text(-1.5, .5, text, transform=axes[0].transAxes, \n",
    "                     verticalalignment='center')\n",
    "    \n",
    "    for a in axgrid[-1]: a.set_xlabel('Epochs')\n",
    "    # General plot properties\n",
    "    fig.subplots_adjust(wspace=.05, hspace=.05)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [(2, 64, 40),\n",
    "           (4, 32, 40),\n",
    "           (2, 32, 40),\n",
    "           (6, 32, 40),\n",
    "           (4, 64, 40),\n",
    "           (1, 64, 40),\n",
    "           (6, 16, 40)]\n",
    "fun_descr = lambda config: f'{config[0]} layers, {config[1]} neurons'\n",
    "plot_statistics(data, configs, [4000, 5000, 6000], \n",
    "                ['loss', 'val_loss', 'mae', 'val_mae'], fun_descr)\n",
    "plt.savefig('figs/dnn_statistics_sorted_40aks.eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort by validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = avg.sort_values('val_loss').reset_index(drop=True)\n",
    "best.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2config(df):\n",
    "    return [\n",
    "        (l, n, c) for l,n,c in zip(df.hlayers, df.neurons, df.aks)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2config(best.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best[best.aks == 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conmpare effect of aks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the assumed best architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df2config(best.head(1))[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = avg[np.logical_and(avg.hlayers == 2, avg.neurons == 64)].sort_values('val_loss')\n",
    "configs = df2config(sub)\n",
    "fun_descr = lambda config: f'L={config[0]}, N={config[1]}, C={config[2]}'\n",
    "plot_statistics(data, configs, [4000, 5000, 6000], \n",
    "                ['loss', 'val_loss', 'mae', 'val_mae'], fun_descr)\n",
    "plt.savefig('figs/dnn_statistics_2layers_64neurons.eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort irrespectively of the number of coefs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = df2config(best.head(10))\n",
    "fun_descr = lambda config: f'L={config[0]}, N={config[1]}, C={config[2]}'\n",
    "plot_statistics(data, configs, [4000, 5000, 6000], \n",
    "                ['loss', 'val_loss', 'mae', 'val_mae'], fun_descr)\n",
    "plt.savefig('figs/dnn_statistics_best.eps', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
